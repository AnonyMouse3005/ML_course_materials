{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Sentiment Analysis\n",
    "\n",
    "A tweet or a review can often tell us how its author feels about the subject of the text. In this homework, we will use the [IMDB review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to automatically infer for a given review whether it's positive, hence its author liked the movie, or negative. This subtype of text classification which aims to infer the sentiments of the text's author is **sentiment analysis**.\n",
    "\n",
    "Naive Bayes is a probabilistic classifier which assigns among all classes the class with highest probability to a given instance. In our case, we will use it to classify movie review texts into a positive and a negative class. For a given text or document $d$ Naive Bayes decides on the class $\\hat{c}$ among all classes $C$ according to the formula:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{c} &= \\underset{c\\in C}{\\operatorname{argmax}} P(c \\mid d)\\\\\n",
    "&=\\underset{c\\in C}{\\operatorname{argmax}}\\frac{P(d \\mid c)P(c)}{P(d)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "$P(d)$ is the same for all classes and will not affect which class is more probable, we can disregard it. We will need to calculate the other two quantities likelihood $P(d \\mid c)$ and class prior probabilities $P(c)$. \n",
    "\n",
    "A movie review $d$ can be represented by features $w_i$ where $i$ indicates word position. Hence, we can rewrite our likelihood for document $d$ with $n$ words as\n",
    "\n",
    "\\begin{align}\n",
    "P(d \\mid c) = P(w_1, w_2, w_3,\\dots, w_n \\mid c)\n",
    "\\end{align}\n",
    "\n",
    "We are using the **bag-of-words** assumption. That is, we only care about the words themselves but not their positions. A model with this assumption would evaluate \"Apple likes you.\" and \"You like apples.\" the same after some common text preprocessing steps. Also, we will make the <b>naive Bayes assumption</b> which simplifies the calculation of the likelihood:\n",
    "\n",
    "\\begin{align}\n",
    "P(d \\mid c) &= P(w_1, w_2, w_3,\\dots, w_n \\mid c)\\\\ \n",
    "&=P(w_1 \\mid c)P(w_2 \\mid c) \\dots P(w_n \\mid c)\n",
    "\\end{align}\n",
    "\n",
    "We will determine the likelihood of the word in a given position $i$ using its count (bag-of-words model). Using the naive Bayes assumption, we will count the occurence of single words (**unigrams**), e.g. \"apples\". **Bigram**, e.g. \"like apples\", **trigram** or higher order models violate the naive Bayes assumption. The likelihood will be the fraction of all words in class $c$ that are the same as word $w_i$.\n",
    "\n",
    "**Note**: For a given word $w_i$ = \"apple\", $P(w_i = \\text{\"apple\"} \\mid c)$ is calculated by counting the occurence of \"apple\" in **all** documents in class $c$. \n",
    "\n",
    "Our final class decision will be made by:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{c} &= \\underset{c\\in C}{\\operatorname{argmax}}P(c)\\underset{0<i\\leq n}{\\operatorname{\\Pi}}P(w_i \\mid c)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections of The Notebook\n",
    "\n",
    "1. [Reading Training and Test Sets](#data)<br>\n",
    "    1.1 [Summary of Dataset](#dataset-summary)<br>\n",
    "    1.2 [An Example Review](#example-review)<br>\n",
    "2. [Classification with scikit-learn](#sklearn)<br>\n",
    "    2.1 [CountVectorizer](#count-vectorizer)<br>\n",
    "    2.2 [MultinomialNB](#multinomialNB) <br>\n",
    "    2.3 [Sklearn Pipeline](#sklearn-pipeline)<br>\n",
    "3. [Processing with SpaCy Language Model](#spacy)<br>\n",
    "4. [Sklearn with SpaCy Lemmatization and Vocabulary](#sklearn-with-spacy)\n",
    "5. [Multinomial Naive Bayes Implementation](#multinomialnb-implementation)\n",
    "6. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## 1. Reading Traning and Test Sets\n",
    "\n",
    "We will read in the training and testing examples and labels. The examples in this dataset are review texts with some html occasionally. The label information will be deduced from the folder name. Read more about the dataset and folder organization in [README](#dataset-summary).\n",
    "\n",
    "**Please be aware that the data format and organization will be somewhat different from dataset to dataset. Running the code below as it is will not work on most other datasets.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir data && curl https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz > data/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tarfile is needed to read from a tar archive\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path specification for the files\n",
    "ARCHIVE_NAME = \"./data/aclImdb_v1.tar.gz\"\n",
    "README_NAME = \"aclImdb/README\"\n",
    "TRAIN_FOLDER = \"train/\"\n",
    "TEST_FOLDER = \"test/\"\n",
    "POSITIVE_FOLDER = \"pos/\"\n",
    "NEGATIVE_FOLDER = \"neg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definitions for reading data from archive and\n",
    "# populating training and test sets\n",
    "\n",
    "\n",
    "def text_decoder(text):\n",
    "    \"\"\"Decodes to utf-8\"\"\"\n",
    "    return text.decode()\n",
    "\n",
    "\n",
    "def truncate_to(text, pos=1000):\n",
    "    \"\"\"Show text until the character at pos index\"\"\"\n",
    "    return text[:pos]\n",
    "\n",
    "\n",
    "def archive_file_contents(tar, info):\n",
    "    \"\"\"Get contents of tar file\"\"\"\n",
    "    f = tar.extractfile(info)\n",
    "    return f.read()\n",
    "\n",
    "\n",
    "def get_raw_data_from(tar):\n",
    "    \"\"\"Reads in the tar archive, forms the training and test set\"\"\"\n",
    "\n",
    "    train_reviews = []\n",
    "    train_labels = []\n",
    "    test_reviews = []\n",
    "    test_labels = []\n",
    "\n",
    "    # for each file in the archive,\n",
    "    # get the filename and tarinfo of the compressed file\n",
    "    for fname, farchive in zip(tar.getnames(), tar.getmembers()):\n",
    "        # if the file is in the training set\n",
    "        if TRAIN_FOLDER in fname:\n",
    "            # and a positive review\n",
    "            if POSITIVE_FOLDER in fname:\n",
    "                # add the review to the train_reviews list\n",
    "                train_reviews.append(text_decoder(archive_file_contents(tar, farchive)))\n",
    "                # add a 1 to the train_labels list indicating the example is a positive review\n",
    "                train_labels.append(1)\n",
    "\n",
    "            # if the file is a negative review\n",
    "            if NEGATIVE_FOLDER in fname:\n",
    "                # add the review to the train_reviews list\n",
    "                train_reviews.append(text_decoder(archive_file_contents(tar, farchive)))\n",
    "                # add a 0 to the train_labels list indicating the example is a negative review\n",
    "                train_labels.append(0)\n",
    "\n",
    "        # if the file is in the test set\n",
    "        elif TEST_FOLDER in fname:\n",
    "            # and a positive review\n",
    "            if POSITIVE_FOLDER in fname:\n",
    "                # add the review to the test_reviews list\n",
    "                test_reviews.append(text_decoder(archive_file_contents(tar, farchive)))\n",
    "                # add a 1 to the test_labels list indicating the example is a positive review\n",
    "                test_labels.append(1)\n",
    "\n",
    "            # if the file is a negative review\n",
    "            if NEGATIVE_FOLDER in fname:\n",
    "                # add the review to the test_reviews list\n",
    "                test_reviews.append(text_decoder(archive_file_contents(tar, farchive)))\n",
    "                # add a 0 to the test_labels list indicating the example is a negative review\n",
    "                test_labels.append(0)\n",
    "\n",
    "    return train_reviews, train_labels, test_reviews, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(ARCHIVE_NAME, \"r:gz\") as tar:\n",
    "    X_train, y_train, X_test, y_test = get_raw_data_from(tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset-summary\"></a>\n",
    "### 1.1 Summary of Dataset\n",
    "\n",
    "Information about the dataset from the included README file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. \n"
     ]
    }
   ],
   "source": [
    "with tarfile.open(ARCHIVE_NAME, \"r:gz\") as tar:\n",
    "    readme = text_decoder(archive_file_contents(tar, README_NAME))\n",
    "    # only the first 1761 characters are informative for us\n",
    "    print(truncate_to(readme, 1761))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-review\"></a>\n",
    "### 1.2 An Example Review\n",
    "\n",
    "To have an idea of what the reviews look like, we can print the contents of a random review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have watched this film twice now and think its quite good for the limited equipment used to create this film. (filmed in 1947) Dr. Heyerdahl explains his theory about the migration of south American Pre-Colubian Indians to the Polynesia's islands by way raft fell of large balsa trees. This documentary follows Dr. Heyerdahl and crew as they select balsa trees in Equidor and float with them down river to the pacific for assembly in Peru. They launch off on a 101 day sea adventure testing the strength of their primitive raft surviving only by means available to natives of that era. See for yourself, a real adventure!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# setting a seed makes sure we get consistent results across runs\n",
    "random.seed(142)\n",
    "ind = random.randint(a=0, b=len(X_train))\n",
    "review_example = X_train[ind]\n",
    "print(review_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sklearn\"></a>\n",
    "## 2. Classification with sklearn\n",
    "\n",
    "Scikit-learn ([sklearn](https://scikit-learn.org/stable/)) is a powerful library that simplifies machine learning. Most of the well-known machine learning algorithms are implemented as well as methods for hyperparameter selection, model evaluation, etc. We will use the feature extraction module to calculate word counts, metrics module to calculate accuracies, naive_bayes module for the classification algorithm, and finally pipeline module to put parts of our model together to run more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"count-vectorizer\"></a>\n",
    "### 2.1 CountVectorizer\n",
    "\n",
    "CountVectorizer returns the number of occurrences of words in the documents. It can take several arguments, which are listed in [its documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). In this section, we will use it with its default arguments. Later, we will specify the vocabulary for which the counts will be calculated, and some other parameters. When the vocabulary is not explicitly specified, it's constructed from the union of all words in all documents.\n",
    "\n",
    "The shape of the returned counts will be $n_{\\text{docs}} \\times n_{\\text{vocab}}$, where $n_{\\text{docs}}$ is the number of documents (in this case just the `review_example`, hence 1), and $n_{\\text{vocab}}$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('have', 29),\n",
       " ('watched', 75),\n",
       " ('this', 70),\n",
       " ('film', 23),\n",
       " ('twice', 73),\n",
       " ('now', 42),\n",
       " ('and', 5),\n",
       " ('think', 69),\n",
       " ('its', 35),\n",
       " ('quite', 52)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "example_counts = vectorizer.fit_transform(\n",
    "    [review_example]\n",
    ")  # CountVectorizer object expects an iterable such as a list\n",
    "\n",
    "# CountVectorizer returns a sparse representation.\n",
    "# To see its contents, we will need to cast it\n",
    "# to a dense array.\n",
    "example_counts_dense = example_counts.toarray()\n",
    "\n",
    "# vocabulary is a one-to-one mapping between words and integers\n",
    "# Each of these integers specify the index of the corresponding word in the count array returned\n",
    "\n",
    "list(vectorizer.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 79 distinct words in the review_example.\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(list(vectorizer.vocabulary_.items()))\n",
    "print(f\"There are {n_vocab} distinct words in the review_example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check which words were the most frequent in `review_example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent word is 'the' with 5 occurences.\n"
     ]
    }
   ],
   "source": [
    "# Let's find the most frequent word and how many times it occurs in the example review\n",
    "reverse_vocabulary_lookup = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "len(list(reverse_vocabulary_lookup.items()))\n",
    "\n",
    "print(\n",
    "    f\"The most frequent word is '{reverse_vocabulary_lookup[np.argmax(example_counts_dense)]}' with {np.max(example_counts_dense)} occurences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we gaining any information from the most frequent word? If you change the seed for the random number generator or the index into the training set in [example review section](#example-review), you will see that the most frequent words in the documents are words like \"the\", \"and\" which are not informative. These words are called **stop words**, we will clean the documents from stop words in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=multinomialNB></a>\n",
    "### 2.2 MultinomialNB\n",
    "\n",
    "For our problem, we can use the MultinomialNB (multinomial naive Bayes) implemented in `sklearn`. First, we will need to fit and transform our training data with the `CountVectorizer` instance. Then, we will fit the MultinomialNB classifier to our training dataset, i.e., we will train our MultinomialNB classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = vectorizer.fit_transform(X_train)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts = vectorizer.fit_transform(X_test)\n",
    "\n",
    "# If you run the two lines of code below you will get a dimension mismatch error.\n",
    "# See the text before Section 1.3.\n",
    "\n",
    "# test_predictions = classifier.predict(test_counts)\n",
    "# accuracy_score(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 25000 reviews with a vocabulary of size 74849.\n",
      "Test set has 25000 reviews with a vocabulary of size 73822.\n"
     ]
    }
   ],
   "source": [
    "n_train_examples, n_train_corpus = train_counts.shape\n",
    "n_test_examples, n_test_corpus = test_counts.shape\n",
    "print(\n",
    "    f\"Training set has {n_train_examples} reviews with a vocabulary of size {n_train_corpus}.\"\n",
    ")\n",
    "print(\n",
    "    f\"Test set has {n_train_examples} reviews with a vocabulary of size {n_test_corpus}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different vocabularies in the training and test sets lead to \"dimension mismatch error\". We can fix a vocabulary beforehand and disregard words not in that vocabulary in all of the computations. Instead of specifying the words in the vocabulary explicitly, we will use **SpaCy**'s English model vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sklearn-pipeline\"></a>\n",
    "### 2.3 Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn allows us to define a custom pipeline where each component of the pipeline can be custom. The inputs to the components are the outputs of the previous component in the pipeline. The code below is equivalent to classification steps in the previous [CountVectorizer](#count-vectorizer) and [MultinomialNB](#multinomialNB) sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"vectorize\", CountVectorizer()), (\"clf\", MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorize', CountVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will predict the test set classes and report model accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.81356\n"
     ]
    }
   ],
   "source": [
    "predicted = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, predicted)\n",
    "print(f\"Test accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"spacy\"></a>\n",
    "## 3. Processing with SpaCy Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy is a Natural Language Processing library in Python. It offers a lot of functionality. We will use its lemmatization method in this notebook for preprocessing the reviews.\n",
    "\n",
    "We will need to load the English language model. It is in the `shared` folder of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import the SpaCy library\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm-2.3.1\", disable=[\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture the similarities between words that come from the same root, such as \"apple\" vs \"apples\" and \"bring\" vs \"brought\", we can either stem or lemmatize the words. Stemming doesn't always result in actual words, for example \"brought\" might end up as \"brough\". Lemmatization produces whole actual words. We will see examples of lemmatization below, and later use lemmatization in our sklearn pipeline. You can read more about [SpaCy Tokens](https://spacy.io/api/token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watched                       watch\n",
      "film                          film\n",
      "twice                         twice\n",
      "think                         think\n",
      "good                          good\n",
      "limited                       limit\n",
      "equipment                     equipment\n",
      "create                        create\n",
      "film                          film\n",
      "filmed                        film\n",
      "1947                          1947\n",
      "Dr.                           Dr.\n",
      "Heyerdahl                     Heyerdahl\n",
      "explains                      explain\n",
      "theory                        theory\n",
      "migration                     migration\n",
      "south                         south\n",
      "American                      American\n",
      "Pre                           Pre\n",
      "Colubian                      Colubian\n",
      "Indians                       Indian\n",
      "Polynesia                     Polynesia\n",
      "islands                       island\n",
      "way                           way\n",
      "raft                          raft\n",
      "fell                          fall\n",
      "large                         large\n",
      "balsa                         balsa\n",
      "trees                         tree\n",
      "documentary                   documentary\n",
      "follows                       follow\n",
      "Dr.                           Dr.\n",
      "Heyerdahl                     Heyerdahl\n",
      "crew                          crow\n",
      "select                        select\n",
      "balsa                         balsa\n",
      "trees                         tree\n",
      "Equidor                       Equidor\n",
      "float                         float\n",
      "river                         river\n",
      "pacific                       pacific\n",
      "assembly                      assembly\n",
      "Peru                          Peru\n",
      "launch                        launch\n",
      "101                           101\n",
      "day                           day\n",
      "sea                           sea\n",
      "adventure                     adventure\n",
      "testing                       test\n",
      "strength                      strength\n",
      "primitive                     primitive\n",
      "raft                          raft\n",
      "surviving                     survive\n",
      "means                         mean\n",
      "available                     available\n",
      "natives                       native\n",
      "era                           era\n",
      "real                          real\n",
      "adventure                     adventure\n"
     ]
    }
   ],
   "source": [
    "# SpaCy models come with taggers, parsers and named-entity-recognizers.\n",
    "# We will disable the parts of the spacy processing pipeline that we will not need.\n",
    "# doc will be a series of SpaCy Tokens\n",
    "\n",
    "doc = nlp(review_example, disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "# You can think about a SpaCy token\n",
    "# as a word in the doc with extra information\n",
    "# coming from SpaCy processing\n",
    "# like position in document, lemma, etc.\n",
    "\n",
    "# For each token in the document\n",
    "for token in doc:\n",
    "    # if the token is not a stop word or a punctuation mark\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        # print the token text and its lemmatization\n",
    "        print(f\"{token.text:<30}{token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=sklearn-with-spacy></a>\n",
    "## 4. Sklearn with SpaCy Lemmatization and Vocabulary\n",
    "\n",
    "Finally, we can process the reviews with SpaCy lemmatization and vocabulary in the CountVectorizer. Then, run our MultinomialNB classifier on the output in a sklearn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\"Cleans up html tags\"\"\"\n",
    "    html_regex = \"<[^>]*>*<[^>]*>\"\n",
    "    if type(text) == str:\n",
    "        text = re.sub(html_regex, \"\", text)\n",
    "        text = re.sub(\"[\\W]+\", \"\", text.lower())\n",
    "    return text\n",
    "\n",
    "\n",
    "# tokenize the doc, remove stop words and lemmatize its tokens\n",
    "def tokenize(doc):\n",
    "    tokens = nlp(doc)\n",
    "    return [preprocessor(token.lemma_) for token in tokens if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline below will take significantly longer than the above pipeline where we had kept the default settings for the CountVectorizer. SpaCy can be optimized to run faster with nlp.pipe and processing all of the documents before inputting to CountVectorizer. We will not cover nlp.pipe here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x7ff372bad310>,\n",
       "                                 vocabulary=<spacy.strings.StringStore object at 0x7ff372cb12c0>)),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                vocabulary=nlp.vocab.strings, tokenizer=tokenize, ngram_range=(1, 1)\n",
    "            ),\n",
    "        ),\n",
    "        (\"classifier\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "spacy_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61992\n"
     ]
    }
   ],
   "source": [
    "predicted = spacy_pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, predicted)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer also has some processing steps such as converting to lowercase, which is set to True by default, and removing stop words. Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=multinomialnb-implementation></a>\n",
    "## 5. Multinomial Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "class NaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: a sparse matrix of counts\n",
    "            y: Labels array\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "\n",
    "        # In the fit method we'll estimate the parameters of the model.\n",
    "        # In sklearn, fit method in any class extended from BaseEstimator should\n",
    "        # return the object itself.\n",
    "\n",
    "        # In our pipeline, X will be the output of the CountVectorizer instance\n",
    "        # and it is a sparse matrix.\n",
    "\n",
    "        # We want to take out only the non-zero entries\n",
    "        # and their coordinates (in document_number, word_index form)\n",
    "\n",
    "        counts_positive_ = defaultdict(int)\n",
    "        counts_negative_ = defaultdict(int)\n",
    "\n",
    "        # counts_ will be a dictionary of dictionaries\n",
    "        # It has two keys: 0 and 1\n",
    "        # 0 will contain the counts of words in the negative documents\n",
    "        # Similarly, 1 will contain the counts of words in the positive documents\n",
    "\n",
    "        counts_ = {0: counts_negative_, 1: counts_positive_}\n",
    "        class_counts_ = defaultdict(int)\n",
    "\n",
    "        # dictionary that keeps the total number of words in all documents in each class\n",
    "        total_word_counts = {0: 0, 1: 0}\n",
    "\n",
    "        # estimate parameters of the model (P(w_i|c))\n",
    "        # for each word w\n",
    "        for d, w in zip(*X.nonzero()):\n",
    "            # collect counts of w in every document\n",
    "            w_count_in_d = X[d, w]\n",
    "\n",
    "            # update the dictionary entries with the counts\n",
    "            # from X.\n",
    "            counts_[y[d]][w] += w_count_in_d\n",
    "\n",
    "            # We also need the total word counts in both the positive and negative groups\n",
    "            # Here, we are incrementing the total word count for the correct group.\n",
    "            total_word_counts[y[d]] += w_count_in_d\n",
    "\n",
    "        # the prior class probabilities\n",
    "        positive_fraction = sum(y) / len(y)\n",
    "        negative_fraction = 1 - positive_fraction\n",
    "\n",
    "        # log class priors\n",
    "        # max operation is needed to avoid invalid input to log\n",
    "        self.class_priors_ = {\n",
    "            0: np.log(negative_fraction),\n",
    "            1: np.log(positive_fraction),\n",
    "        }\n",
    "\n",
    "        # We will calculate the class conditional probability (likelihood)\n",
    "        # of each word.\n",
    "        all_words = set(list(counts_[0].keys()) + list(counts_[1].keys()))\n",
    "\n",
    "        # We initialize these likelihoods to 0\n",
    "        self.word_probs_ = defaultdict(lambda: {0: 0, 1: 0})\n",
    "\n",
    "        # then for each word, we'll update the word likelihoods\n",
    "        # the likelihood for a word will be the count of that word in all documents in class c\n",
    "        # divided by the count of all words in all documents in class c.\n",
    "        for v in all_words:\n",
    "            self.word_probs_[v][0] = np.log(max(1, counts_[0][v])) - np.log(\n",
    "                total_word_counts[0]\n",
    "            )\n",
    "            self.word_probs_[v][1] = np.log(max(1, counts_[1][v])) - np.log(\n",
    "                total_word_counts[1]\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: a sparse matrix of counts\n",
    "            y: None (kept for compatibility)\n",
    "        Returns:\n",
    "            preds: an array of predicted classes\n",
    "        \"\"\"\n",
    "\n",
    "        # For each document d in X,\n",
    "        # we will use the class priors and likelihoods calculated in fit\n",
    "        # to predict the predicted class for d.\n",
    "\n",
    "        d = 0\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # unnormalized posterior probabilites\n",
    "        prob_pos = self.class_priors_[1]\n",
    "        prob_neg = self.class_priors_[0]\n",
    "\n",
    "        prev_d = d\n",
    "\n",
    "        # The sparse matrix structure of CountVectorizer output X\n",
    "        # allows us to iterate over documents and words in X.\n",
    "\n",
    "        for d, w in zip(*X.nonzero()):\n",
    "\n",
    "            # When we encounter the next document in X,\n",
    "            # append the class prediction for the previous document to preds\n",
    "            # and reset the unnormalized posterior probability to the class prior.\n",
    "            if d != prev_d:\n",
    "                preds.append(1 if prob_pos > prob_neg else 0)\n",
    "                prob_pos = self.class_priors_[1]\n",
    "                prob_neg = self.class_priors_[0]\n",
    "                prev_d = d\n",
    "\n",
    "            # While we are still processing the same document,\n",
    "            # add the word likelihoods to the unnormalized posterior probabilities.\n",
    "            prob_pos += self.word_probs_[w][1]\n",
    "            prob_neg += self.word_probs_[w][0]\n",
    "\n",
    "        # append the class prediction for the last document\n",
    "        preds.append(1 if prob_pos > prob_neg else 0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x7f8f98547af0>,\n",
       "                                 vocabulary=<spacy.strings.StringStore object at 0x7f8f9853f340>)),\n",
       "                ('classifier', NaiveBayes())])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                vocabulary=nlp.vocab.strings, tokenizer=tokenize, ngram_range=(1, 1)\n",
    "            ),\n",
    "        ),\n",
    "        (\"classifier\", NaiveBayes()),\n",
    "    ]\n",
    ")\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82588"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pipeline.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercises></a>\n",
    "## 6. Exercises\n",
    "\n",
    "Another type of naive Bayes model known as **binary multinomial naive Bayes** (or binary naive Bayes) is commonly used for sentiment analysis. In this model, the likelihood $P(w_i \\mid c)$ is given by the fraction of documents in class $c$ that contain $w_i$ rather than the fraction of all words in class $c$ that are the same as $w_i$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Adapt the classifier defined in [Multinomial naive Bayes implementation](#multinomialnb-implementation) to include add-one (Laplace) smoothing and plug into the pipeline instead of the MultinomialNB classifier. Report accuracy on the test set.  You can implement your own classifier from scratch instead of adapting the above.\n",
    "\n",
    "**Hint:** Output of CountVectorizer instance has shape $n_{\\text{doc}} \\times n_{\\text{vocab}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "class NaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: a sparse matrix of counts\n",
    "            y: Labels array\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "\n",
    "        # In the fit method we'll estimate the parameters of the model.\n",
    "        # In sklearn, fit method in any class extended from BaseEstimator should\n",
    "        # return the object itself.\n",
    "\n",
    "        # In our pipeline, X will be the output of the CountVectorizer instance\n",
    "        # and it is a sparse matrix.\n",
    "\n",
    "        # We want to take out only the non-zero entries\n",
    "        # and their coordinates (in document_number, word_index form)\n",
    "\n",
    "        counts_positive_ = defaultdict(int)\n",
    "        counts_negative_ = defaultdict(int)\n",
    "\n",
    "        # counts_ will be a dictionary of dictionaries\n",
    "        # It has two keys: 0 and 1\n",
    "        # 0 will contain the counts of words in the negative documents\n",
    "        # Similarly, 1 will contain the counts of words in the positive documents\n",
    "\n",
    "        counts_ = {0: counts_negative_, 1: counts_positive_}\n",
    "        class_counts_ = defaultdict(int)\n",
    "\n",
    "        # dictionary that keeps the total number of words in all documents in each class\n",
    "        total_word_counts = {0: 0, 1: 0}\n",
    "\n",
    "        # estimate parameters of the model (P(w_i|c))\n",
    "        # for each word w\n",
    "        for d, w in zip(*X.nonzero()):\n",
    "            # collect counts of w in every document\n",
    "            w_count_in_d = X[d, w]\n",
    "\n",
    "            # update the dictionary entries with the counts\n",
    "            # from X.\n",
    "            counts_[y[d]][w] += w_count_in_d\n",
    "\n",
    "            # We also need the total word counts in both the positive and negative groups\n",
    "            # Here, we are incrementing the total word count for the correct group.\n",
    "            total_word_counts[y[d]] += w_count_in_d\n",
    "\n",
    "        # the prior class probabilities\n",
    "        alpha = 0.8\n",
    "        n_features = X.nonzero()[1].shape[0]\n",
    "        # n_features = 2\n",
    "        print(n_features)\n",
    "        positive_fraction = (sum(y) + alpha) / (len(y) + n_features*alpha)\n",
    "        negative_fraction = 1 - positive_fraction\n",
    "\n",
    "        # log class priors\n",
    "        # max operation is needed to avoid invalid input to log\n",
    "        self.class_priors_ = {\n",
    "            0: np.log(negative_fraction),\n",
    "            1: np.log(positive_fraction),\n",
    "        }\n",
    "\n",
    "        # We will calculate the class conditional probability (likelihood)\n",
    "        # of each word.\n",
    "        all_words = set(list(counts_[0].keys()) + list(counts_[1].keys()))\n",
    "\n",
    "        # We initialize these likelihoods to 0\n",
    "        self.word_probs_ = defaultdict(lambda: {0: 0, 1: 0})\n",
    "\n",
    "        # then for each word, we'll update the word likelihoods\n",
    "        # the likelihood for a word will be the count of that word in all documents in class c\n",
    "        # divided by the count of all words in all documents in class c.\n",
    "        for v in all_words:\n",
    "            self.word_probs_[v][0] = np.log(max(1, counts_[0][v])) - np.log(\n",
    "                total_word_counts[0]\n",
    "            )\n",
    "            self.word_probs_[v][1] = np.log(max(1, counts_[1][v])) - np.log(\n",
    "                total_word_counts[1]\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: a sparse matrix of counts\n",
    "            y: None (kept for compatibility)\n",
    "        Returns:\n",
    "            preds: an array of predicted classes\n",
    "        \"\"\"\n",
    "\n",
    "        # For each document d in X,\n",
    "        # we will use the class priors and likelihoods calculated in fit\n",
    "        # to predict the predicted class for d.\n",
    "\n",
    "        d = 0\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # unnormalized posterior probabilites\n",
    "        prob_pos = self.class_priors_[1]\n",
    "        prob_neg = self.class_priors_[0]\n",
    "\n",
    "        prev_d = d\n",
    "\n",
    "        # The sparse matrix structure of CountVectorizer output X\n",
    "        # allows us to iterate over documents and words in X.\n",
    "\n",
    "        for d, w in zip(*X.nonzero()):\n",
    "\n",
    "            # When we encounter the next document in X,\n",
    "            # append the class prediction for the previous document to preds\n",
    "            # and reset the unnormalized posterior probability to the class prior.\n",
    "            if d != prev_d:\n",
    "                preds.append(1 if prob_pos > prob_neg else 0)\n",
    "                prob_pos = self.class_priors_[1]\n",
    "                prob_neg = self.class_priors_[0]\n",
    "                prev_d = d\n",
    "\n",
    "            # While we are still processing the same document,\n",
    "            # add the word likelihoods to the unnormalized posterior probabilities.\n",
    "            prob_pos += self.word_probs_[w][1]\n",
    "            prob_neg += self.word_probs_[w][0]\n",
    "\n",
    "        # append the class prediction for the last document\n",
    "        preds.append(1 if prob_pos > prob_neg else 0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82588\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct the pipeline. \n",
    "\n",
    "spacy_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                vocabulary=nlp.vocab.strings, tokenizer=tokenize, ngram_range=(1, 1)\n",
    "            ),\n",
    "        ),\n",
    "        (\"classifier\", NaiveBayes()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the training set, make predictions for the test set and report accuracy.\n",
    "spacy_pipeline.fit(X_train, y_train)\n",
    "\n",
    "predicted = spacy_pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, predicted)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including add-one smoothing in `NaiveBayes()` (alpha=1, n_features=2) does not have an effect on the accuracy on test set (still 0.825). Lowering alpha seems to have no effect on the accuracy, but when we set n_features = # of distinct words, the accuracy drops to 0.73."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove duplicates from each review (`doc`). Remember `doc` variable is an object of type `spacy.Tokens.Doc`.  It is a sequence of `Token` objects. A token in SpaCy is more than a simple text and tokens for the same text are not equal to each other. You will need to compare either the `text` or better yet the `lemma_` attributes of the tokens.\n",
    "\n",
    "#### After removing the duplicates, use the above pipeline (`spacy_pipeline`) to build a binary multinomial naive Bayes model. Report accuracy on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watch',\n",
       " 'film',\n",
       " 'twice',\n",
       " 'think',\n",
       " 'good',\n",
       " 'limit',\n",
       " 'equipment',\n",
       " 'create',\n",
       " 'film',\n",
       " '1947',\n",
       " 'dr',\n",
       " 'heyerdahl',\n",
       " 'explain',\n",
       " 'theory',\n",
       " 'migration',\n",
       " 'south',\n",
       " 'american',\n",
       " 'pre',\n",
       " 'colubian',\n",
       " 'indian',\n",
       " 'polynesia',\n",
       " 'island',\n",
       " 'way',\n",
       " 'raft',\n",
       " 'fall',\n",
       " 'large',\n",
       " 'balsa',\n",
       " 'tree',\n",
       " 'documentary',\n",
       " 'follow',\n",
       " 'crow',\n",
       " 'select',\n",
       " 'equidor',\n",
       " 'float',\n",
       " 'river',\n",
       " 'pacific',\n",
       " 'assembly',\n",
       " 'peru',\n",
       " 'launch',\n",
       " '101',\n",
       " 'day',\n",
       " 'sea',\n",
       " 'adventure',\n",
       " 'test',\n",
       " 'strength',\n",
       " 'primitive',\n",
       " 'survive',\n",
       " 'mean',\n",
       " 'available',\n",
       " 'native',\n",
       " 'era',\n",
       " 'real']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for token in nlp(review_example):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83008\n"
     ]
    }
   ],
   "source": [
    "# update the tokenize function to remove duplicates from each review (doc)\n",
    "# it should return a list of lemmas as in the previous definition.\n",
    "# A better way would be to define a preprocessor and pass it to the preprocessor\n",
    "# parameter of CountVectorizer. To keep things simpler we will not be doing that\n",
    "# here.\n",
    "\n",
    "def tokenize(doc):\n",
    "    tokens = list({keyword.__repr__(): keyword for keyword in nlp(doc)}.values())  # remove duplicated tokens\n",
    "    return [preprocessor(token.lemma_) for token in tokens if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "# Reconstruct the pipeline. \n",
    "\n",
    "spacy_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                vocabulary=nlp.vocab.strings, tokenizer=tokenize, ngram_range=(1, 1)\n",
    "            ),\n",
    "        ),\n",
    "        (\"classifier\", NaiveBayes()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the training set, make predictions for the test set and report accuracy.\n",
    "spacy_pipeline.fit(X_train, y_train)\n",
    "\n",
    "predicted = spacy_pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, predicted)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing duplicates using `spacy.util.filter_spans` function, we observe a slight increase in accuracy in the test set from 0.825 to 0.830."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "Please contact Zeynep Hakguder (<a href=\"mailto:zphakguder@gmail.com\">zphakguder@gmail.com</a>) for further questions or inquries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('hw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b36cb32713be4a515554d614b20db2b63dff7b3d0f97a523ccc0881bb4fdc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
